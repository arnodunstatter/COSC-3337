{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32clx5u2uABe"
   },
   "source": [
    "The following approach was original inspired by this [tutorial](https://www.youtube.com/watch?v=FB5EdxAGxQg&ab_channel=codebasics) - however it was later realized that due to the quadratic time complexity the [sklearn.svm.SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) may not be right for our project, since it does not support Stochastic Gradient Descent using batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aA_GjHZAHM9I"
   },
   "source": [
    "##**Loading the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cU0LqFjYlGmf",
    "outputId": "7a70abbe-ed2d-4f29-ceb5-15c1bbf1163e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch\n",
    "from random import seed\n",
    "seed(40)\n",
    "torch.manual_seed(40)\n",
    "\n",
    "# Transformations applied to images as they are loaded by the Pytorch dataloader\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "#where we'll store the data\n",
    "google_drive_path = './drive/MyDrive/Colab Notebooks/3337_Project'\n",
    "\n",
    "#download our data \n",
    "trainset = torchvision.datasets.CIFAR10(root=google_drive_path, train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root=google_drive_path, train=False, download=True, transform=transform)\n",
    "#the above code is really just used for DLing all data\n",
    "#we still need to manually transform it (normalize it)\n",
    "#and we still need to split it into training and testing data\n",
    "\n",
    "#don't need these anymore\n",
    "del trainset \n",
    "del testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TuwsGZ8yHPmo"
   },
   "source": [
    "##**Defining functions for loading the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-LTN4iSsl8HK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(40)\n",
    "\n",
    "def unpickle(file): #adapted from https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "  \"\"\" load single batch of cifar \"\"\"\n",
    "  datadict = unpickle(filename)\n",
    "  X = datadict[b'data']\n",
    "  Y = datadict[b'labels']\n",
    "  X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "  Y = np.array(Y)\n",
    "  return X, Y\n",
    "\n",
    "def load_CIFAR10(ROOT): #adapted from tutorial #this splits the data into training and testing sets\n",
    "  \"\"\" load all of cifar \"\"\"\n",
    "  xs = []\n",
    "  ys = []\n",
    "  for b in range(1,6):\n",
    "    f = ROOT + '/data_batch_%d'%(b,)\n",
    "    X, Y = load_CIFAR_batch(f)\n",
    "    xs.append(X)\n",
    "    ys.append(Y)    \n",
    "  Xtr = np.concatenate(xs)\n",
    "  Ytr = np.concatenate(ys)\n",
    "  del X, Y\n",
    "  Xte, Yte = load_CIFAR_batch(ROOT + '/test_batch')\n",
    "  return Xtr, Ytr, Xte, Yte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsmkulkPHTqW"
   },
   "source": [
    "##**Loading the data and preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gUvBdBROnzdf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cifar10_dir = './drive/MyDrive/Colab Notebooks/3337_Project/cifar-10-batches-py' #changed\n",
    "\n",
    "X_train_4D_unNorm, y_train, X_test_4D_unNorm, y_test = load_CIFAR10(cifar10_dir) #splits into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nFqhhQJkeeB3",
    "outputId": "095aa346-1e67-4bda-e600-a1339e865a81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000,)\n",
      "(10000, 32, 32, 3)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "data = [X_train_4D_unNorm, y_train, X_test_4D_unNorm, y_test]\n",
    "for each in data:\n",
    "  print(each.shape)\n",
    "#still need to reshape and normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "yfMBYSbQzfCG"
   },
   "outputs": [],
   "source": [
    "#reshape the data\n",
    "X_train_2D_unNorm = np.reshape(X_train_4D_unNorm, (X_train_4D_unNorm.shape[0],-1))\n",
    "X_test_2D_unNorm = np.reshape(X_test_4D_unNorm, (X_test_4D_unNorm.shape[0],-1))\n",
    "\n",
    "#now we don't need the 4D data anymore\n",
    "del X_train_4D_unNorm\n",
    "del X_test_4D_unNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0QiKHG0xeL8T",
    "outputId": "87de59d5-5fb3-486b-c204-045614d2d25f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072)\n",
      "(10000, 3072)\n"
     ]
    }
   ],
   "source": [
    "#here we show that we successfully transformed the 4D set of images to a 2D set of images\n",
    "#each column is named column,row,rgb\n",
    "print(X_train_2D_unNorm.shape)\n",
    "print(X_test_2D_unNorm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "o1WvbG_8lyvl"
   },
   "outputs": [],
   "source": [
    "#now let's produce our pd.DataFrames\n",
    "\n",
    "columnLabels = [] #we'll feed this to our pd.DataFrame to label each column for each image\n",
    "for i in range(32):\n",
    "  for j in range(32):\n",
    "    for color in ['r','g','b']:\n",
    "      columnLabels.append(str(i)+','+str(j)+','+color)\n",
    "\n",
    "X_train_unNorm = pd.DataFrame(X_train_2D_unNorm, columns=columnLabels)\n",
    "X_test_unNorm = pd.DataFrame(X_test_2D_unNorm, columns=columnLabels)\n",
    "\n",
    "#don't need X_train_2D_unNorm or X_test_2D_unNorm anymore\n",
    "del X_train_2D_unNorm\n",
    "del X_test_2D_unNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvPFu3B5tiTn"
   },
   "source": [
    "###**Normalization**\n",
    "We are told in the [sklearn.linear_model.SGDClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier) that \"For best results using the default learning rate schedule, the data should have zero mean and unit variance.\" so we will standardize using [sklearn.preprocessing.StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5NTi-rF3tXxU"
   },
   "outputs": [],
   "source": [
    "#let's standardize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train = pd.DataFrame(StandardScaler().fit_transform(X_train_unNorm), columns=columnLabels)\n",
    "#fit_transform uses one argument both to determine the mins and maxs (scaling params) and to do the transform\n",
    "#to normalize our X_test, we want to use the mins and maxs from the train data, since it's such a bigger sample, so we'll do this normalization in steps, first we'll call fit on the training data, then we'll call transform on the test data\n",
    "X_test = pd.DataFrame(StandardScaler().fit(X_train_unNorm).transform(X_test_unNorm), columns=columnLabels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fH5DDPhdyF1D"
   },
   "source": [
    "##**Training**\n",
    "Found this [discussion](https://stackoverflow.com/questions/24617356/sklearn-sgdclassifier-partial-fit) helpful. Note that the SGDClassifier parameter shuffle being set to True apparently does not actually produce shuffling. Shuffling was not implemented manually, but it could be with more time.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "K0Lqq7cQRoWT"
   },
   "outputs": [],
   "source": [
    "#now we are ready to make our Support Vector Classifier (a type of support vector machine)\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import random\n",
    "\n",
    "model = SGDClassifier()\n",
    "for i in range(10):\n",
    "  model.partial_fit(X_train[0:5000*(i+1)], y_train[0:5000*(i+1)], classes=np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PEFlzY5R4fUZ",
    "outputId": "f6f380ec-e4ec-4735-c879-383504aceeee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/drive/MyDrive/Colab Notebooks/3337_Project/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "#here we save the model\n",
    "import pickle\n",
    "pickle_filename = \"./drive/MyDrive/Colab Notebooks/3337_Project/linearSVM_trained_with_batches.pkl\"\n",
    "pickle.dump(model, open(pickle_filename,'wb'))\n",
    "%ls '/drive/MyDrive/Colab Notebooks/3337_Project/' #show that it saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3lOiHVPmzPq"
   },
   "source": [
    "##**Scoring**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yO0n5NLe2QZc",
    "outputId": "cdf60b5c-5f23-4c7a-e838-f7d9d7ec842f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3997"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bWEV6b4cKS-2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "3337_Project_linearSVM_miniBatch_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
